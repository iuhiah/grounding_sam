{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LHaiHui\\AppData\\Local\\anaconda3\\envs\\grounding_sam_cpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import whisper\n",
    "import whisper\n",
    "# import groundingdino\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "# bounding box imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "import matplotlib.pyplot as plt\n",
    "# sam imports\n",
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WHISPER\n",
    "def speechToText(audio_path):\n",
    "    model = whisper.load_model(\"base.en\")\n",
    "    print(\"Whisper loaded.\")\n",
    "    \n",
    "    AUDIO_PATH = audio_path\n",
    "    result = model.transcribe(AUDIO_PATH)\n",
    "    prompt = result[\"text\"]\n",
    "    print(\"Prompt: \", prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GROUNDINGDINO\n",
    "def getBoundingBox(prompt, image_path, output_path, tasks):\n",
    "    model = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"../GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
    "    print(\"GroundingDINO loaded.\")\n",
    "\n",
    "    IMAGE_PATH = image_path\n",
    "    TEXT_PROMPT = prompt\n",
    "    BOX_TRESHOLD = 0.35\n",
    "    TEXT_TRESHOLD = 0.25\n",
    "\n",
    "    image_source, image = load_image(IMAGE_PATH)\n",
    "    print(\"Image loaded.\")\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    if \"annotate\" in tasks:\n",
    "        annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "        cv2.imwrite(output_path, annotated_frame)\n",
    "        print(\"Box drawn.\")\n",
    "\n",
    "    all_coords = getCoords(image_source=image_source, boxes=boxes)\n",
    "    # get box with highest score\n",
    "    coords = all_coords[0]\n",
    "    print(\"Coordinates: \", coords)\n",
    "    return coords\n",
    "\n",
    "def getCoords(image_source: np.ndarray, boxes: torch.Tensor):\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    return xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAM\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def loadSAM():\n",
    "    sam_checkpoint = \"../SAM/checkpoints/sam_vit_h_4b8939.pth\"\n",
    "    model_type = \"vit_h\"\n",
    "\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # device = \"cuda\" if is_cuda else \"cpu\"\n",
    "    if is_cuda: torch.cuda.empty_cache()\n",
    "\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    # sam.to(device=device)\n",
    "    print(\"SAM loaded.\")\n",
    "    return sam\n",
    "\n",
    "def getObjectMask(sam, image_path, coords):\n",
    "    image_cv2 = cv2.imread(image_path)\n",
    "    image_cv2 = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    predictor = SamPredictor(sam)\n",
    "    predictor.set_image(image_cv2)\n",
    "    print(\"Predictor set.\")\n",
    "\n",
    "    # input_box = np.array(coords)\n",
    "    input_box = coords\n",
    "\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_box[None, :],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_cv2)\n",
    "    show_mask(masks[0], plt.gca())\n",
    "    show_box(input_box, plt.gca())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LHaiHui\\AppData\\Local\\anaconda3\\envs\\grounding_sam_cpu\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundingDINO loaded.\n",
      "Image loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LHaiHui\\AppData\\Local\\anaconda3\\envs\\grounding_sam_cpu\\lib\\site-packages\\transformers\\modeling_utils.py:884: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LHaiHui\\AppData\\Local\\anaconda3\\envs\\grounding_sam_cpu\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:  [1113.5714   454.08368 1153.2848   548.22864]\n"
     ]
    }
   ],
   "source": [
    "### MAIN PROGRAM\n",
    "# filename = input(\"Filename: \")\n",
    "# from_audio = input(\"Use audio file? (Y/N)\")\n",
    "\n",
    "# if from_audio == \"Y\":\n",
    "#     audio_path = \"images/\" + filename + \".mp3\"\n",
    "#     prompt = speechToText(audio_path)\n",
    "# else:\n",
    "#     prompt = input(\"Text prompt: \")\n",
    "\n",
    "filename = \"8259\"\n",
    "prompt = \"person on the left\"\n",
    "\n",
    "# get_mask = input(\"Get mask? (Y/N)\")\n",
    "\n",
    "image_path = filename + \".png\"\n",
    "output_path = filename + \"-annotated.jpg\"\n",
    "\n",
    "coords = getBoundingBox(prompt, image_path, output_path, tasks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM loaded.\n"
     ]
    }
   ],
   "source": [
    "image_cv2 = cv2.imread(image_path)\n",
    "image_cv2 = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sam = loadSAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor set.\n"
     ]
    }
   ],
   "source": [
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image_cv2)\n",
    "print(\"Predictor set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_box = np.array(coords)\n",
    "input_box = coords\n",
    "\n",
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_cv2)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grounding_sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
